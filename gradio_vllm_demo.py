import os
import sys
import time
import gradio as gr
from typing import Optional, List, Tuple

import torch
import gc
from PIL import Image, ImageDraw, ImageFont
import io
import re
import glob
import numpy as np
import zipfile
import shutil


# Add vLLM module directory to import path
ROOT_DIR = os.path.dirname(__file__)
VLLM_DIR = os.path.join(ROOT_DIR, "DeepSeek-OCR-master", "DeepSeek-OCR-vllm")
if VLLM_DIR not in sys.path:
    sys.path.append(VLLM_DIR)

from config import (
    MODEL_PATH,
    TOKENIZER_PATH,
    PROMPT,
    CROP_MODE,
    MAX_CONCURRENCY,
)

from deepseek_ocr import DeepseekOCRForCausalLM
from vllm.model_executor.models.registry import ModelRegistry
from vllm import LLM, SamplingParams
from process.image_process import DeepseekOCRProcessor
from process.ngram_norepeat import NoRepeatNGramLogitsProcessor


llm: Optional[LLM] = None
current_engine_cfg = {
    "max_concurrency": None,
    "gpu_memory_utilization": None,
    "max_model_len": None,
}

# Model size presets (accuracy/speed tradeoff)
size_configs = {
    "Tiny": {"base_size": 512, "image_size": 512, "crop_mode": False},
    "Small": {"base_size": 640, "image_size": 640, "crop_mode": False},
    "Base": {"base_size": 1024, "image_size": 1024, "crop_mode": False},
    "Large": {"base_size": 1280, "image_size": 1280, "crop_mode": False},
    "Gundam (Recommended)": {"base_size": 1024, "image_size": 640, "crop_mode": True},
}


def _ensure_offline_env():
    os.environ.setdefault("HF_HUB_OFFLINE", "1")
    os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")
    os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")
    os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "1")


def _setup_cuda_env():
    os.environ.setdefault("VLLM_USE_V1", "0")
    os.environ.setdefault("CUDA_VISIBLE_DEVICES", "0")
    try:
        if getattr(torch.version, "cuda", None) == "11.8":
            for p in [
                "/usr/local/cuda-11.8/bin/ptxas",
                "/usr/local/cuda/bin/ptxas",
            ]:
                if os.path.exists(p):
                    os.environ["TRITON_PTXAS_PATH"] = p
                    break
    except Exception:
        pass


def init_llm(
    max_concurrency: int,
    gpu_memory_utilization: float,
    max_model_len: int = 8192,
    force_reinit: bool = False,
):
    global llm, current_engine_cfg

    if (
        llm is not None
        and not force_reinit
        and current_engine_cfg.get("max_concurrency") == max_concurrency
        and current_engine_cfg.get("gpu_memory_utilization") == gpu_memory_utilization
        and current_engine_cfg.get("max_model_len") == max_model_len
    ):
        return llm

    # Cleanup previous engine if reconfiguring
    if llm is not None:
        try:
            llm.sleep()
        except Exception:
            pass
        try:
            del llm
        except Exception:
            pass
        llm = None
        try:
            torch.cuda.empty_cache()
        except Exception:
            pass
        try:
            if hasattr(torch.cuda, "ipc_collect"):
                torch.cuda.ipc_collect()
        except Exception:
            pass
        try:
            gc.collect()
        except Exception:
            pass
        try:
            torch.cuda.synchronize()
        except Exception:
            pass
        time.sleep(0.5)

    _ensure_offline_env()
    _setup_cuda_env()

    ModelRegistry.register_model("DeepseekOCRForCausalLM", DeepseekOCRForCausalLM)

    try:
        llm = LLM(
            model=MODEL_PATH,
            tokenizer=TOKENIZER_PATH,
            hf_overrides={"architectures": ["DeepseekOCRForCausalLM"]},
            block_size=256,
            enforce_eager=False,
            trust_remote_code=False,
            max_model_len=max_model_len,
            swap_space=0,
            max_num_seqs=max_concurrency,
            tensor_parallel_size=1,
            gpu_memory_utilization=gpu_memory_utilization,
            disable_mm_preprocessor_cache=True,
        )
    except AssertionError as ae:
        # Retry once after aggressive cleanup to handle vLLM memory profiling assertion
        try:
            torch.cuda.empty_cache()
        except Exception:
            pass
        try:
            if hasattr(torch.cuda, "ipc_collect"):
                torch.cuda.ipc_collect()
        except Exception:
            pass
        try:
            gc.collect()
        except Exception:
            pass
        time.sleep(0.5)
        llm = LLM(
            model=MODEL_PATH,
            tokenizer=TOKENIZER_PATH,
            hf_overrides={"architectures": ["DeepseekOCRForCausalLM"]},
            block_size=256,
            enforce_eager=False,
            trust_remote_code=False,
            max_model_len=max_model_len,
            swap_space=0,
            max_num_seqs=max_concurrency,
            tensor_parallel_size=1,
            gpu_memory_utilization=gpu_memory_utilization,
            disable_mm_preprocessor_cache=True,
        )

    current_engine_cfg = {
        "max_concurrency": max_concurrency,
        "gpu_memory_utilization": gpu_memory_utilization,
        "max_model_len": max_model_len,
    }
    return llm


def process_image(
    image: Image.Image,
    prompt_type: str,
    custom_prompt: str,
    model_size: str,
    crop_mode: bool,
    max_concurrency: int,
    gpu_memory_utilization: float,
    max_tokens: int,
):
    try:
        # Guard empty input
        if image is None:
            return "æœªæ£€æµ‹åˆ°å›¾ç‰‡ï¼Œè¯·å…ˆä¸Šä¼ å›¾ç‰‡åå†ç‚¹å‡»å¤„ç†ã€‚"
        llm_local = init_llm(
            max_concurrency=max_concurrency,
            gpu_memory_utilization=gpu_memory_utilization,
            max_model_len=8192,
        )

        if prompt_type == "Free OCR":
            prompt = "<image>\nFree OCR. "
        elif prompt_type == "Markdown Conversion":
            prompt = "<image>\n<|grounding|>Convert the document to markdown. "
        elif prompt_type == "Custom":
            prompt = f"<image>\n{custom_prompt}"
        else:
            prompt = "<image>\nFree OCR. "

        # Apply size preset
        preset = size_configs.get(model_size, size_configs["Gundam (Recommended)"])
        base_size = preset["base_size"]
        image_size = preset["image_size"]
        # Use current checkbox for cropping (updated by preset change)
        image = image.convert("RGB")
        proc = DeepseekOCRProcessor(image_size=image_size, base_size=base_size)
        image_features = proc.tokenize_with_images(
            images=[image], bos=True, eos=True, cropping=crop_mode
        )

        logits_processors = [
            NoRepeatNGramLogitsProcessor(
                ngram_size=40, window_size=90, whitelist_token_ids={128821, 128822}
            )
        ]

        sampling_params = SamplingParams(
            temperature=0.0,
            max_tokens=max_tokens,
            logits_processors=logits_processors,
            skip_special_tokens=False,
        )

        cache_item = {
            "prompt": prompt,
            "multi_modal_data": {"image": image_features},
        }

        outputs = llm_local.generate(
            [cache_item], sampling_params=sampling_params
        )

        content = outputs[0].outputs[0].text
        return content

    except Exception as e:
        import traceback
        return f"Error: {str(e)}\n\nTraceback:\n{traceback.format_exc()}"

def clean_formula(text: str) -> str:
    formula_pattern = r"\\\[(.*?)\\\]"

    def process_formula(match):
        formula = match.group(1)
        formula = re.sub(r"\\quad\s*\([^)]*\)", "", formula)
        formula = formula.strip()
        return r"\[" + formula + r"\]"

    cleaned_text = re.sub(formula_pattern, process_formula, text)
    return cleaned_text

def re_match(text: str) -> Tuple[List[Tuple[str, str, str]], List[str]]:
    pattern = r"(<\|ref\|>(.*?)<\|/ref\|><\|det\|>(.*?)<\|/det\|>)"
    matches = re.findall(pattern, text, re.DOTALL)
    mathes_other = []
    for a_match in matches:
        mathes_other.append(a_match[0])
    return matches, mathes_other

def re_match_pdf(text: str) -> Tuple[List[Tuple[str, str, str]], List[str], List[str]]:
    pattern = r"(<\|ref\|>(.*?)<\|/ref\|><\|det\|>(.*?)<\|/det\|>)"
    matches = re.findall(pattern, text, re.DOTALL)
    mathes_image = []
    mathes_other = []
    for a_match in matches:
        if '<|ref|>image<|/ref|>' in a_match[0]:
            mathes_image.append(a_match[0])
        else:
            mathes_other.append(a_match[0])
    return matches, mathes_image, mathes_other


def _is_image(path: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„å›¾ç‰‡æ ¼å¼"""
    return os.path.splitext(path)[1].lower() in {".jpg", ".jpeg", ".png", ".webp", ".bmp", ".tiff", ".tif"}


def _list_images_in_dir(dir_path: str) -> list:
    """
    åˆ—å‡ºç›®å½•ä¸­æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶ï¼Œå…¼å®¹ä¸­æ–‡è·¯å¾„ã€‚
    ä½¿ç”¨ os.listdir ä»£æ›¿ glob.glob ä»¥é¿å…ä¸­æ–‡è·¯å¾„é—®é¢˜ã€‚
    """
    if not os.path.isdir(dir_path):
        return []
    try:
        files = os.listdir(dir_path)
        images = []
        for f in files:
            full_path = os.path.join(dir_path, f)
            if os.path.isfile(full_path) and _is_image(full_path):
                images.append(full_path)
        return sorted(images)
    except Exception:
        return []


def process_batch_upload(
    uploaded_files: List[str],
    prompt_type: str,
    custom_prompt: str,
    model_size: str,
    crop_mode: bool,
    max_concurrency: int,
    gpu_memory_utilization: float,
    max_tokens: int,
):
    """å¤„ç†ä¸Šä¼ çš„å¤šå¼ å›¾ç‰‡ï¼ˆæ”¯æŒè¿œç¨‹å®¢æˆ·ç«¯ï¼‰"""
    try:
        if not uploaded_files:
            return "è¯·å…ˆä¸Šä¼ å›¾ç‰‡æ–‡ä»¶", ""
        
        llm_local = init_llm(
            max_concurrency=max_concurrency,
            gpu_memory_utilization=gpu_memory_utilization,
            max_model_len=8192,
        )

        images = []
        valid_paths = []
        for file_path in uploaded_files:
            try:
                image = Image.open(file_path).convert("RGB")
                images.append(image)
                valid_paths.append(file_path)
            except Exception as e:
                print(f"skip file: {file_path} due to error: {e}")

        if not images:
            return "æ²¡æœ‰å¯å¤„ç†çš„æœ‰æ•ˆå›¾ç‰‡æ–‡ä»¶", ""

        if prompt_type == "Free OCR":
            prompt = "<image>\nFree OCR. "
        elif prompt_type == "Markdown Conversion":
            prompt = "<image>\n<|grounding|>Convert the document to markdown. "
        elif prompt_type == "Custom":
            prompt = f"<image>\n{custom_prompt}"
        else:
            prompt = "<image>\nFree OCR. "

        preset = size_configs.get(model_size, size_configs["Gundam (Recommended)"])
        base_size = preset["base_size"]
        image_size = preset["image_size"]
        proc = DeepseekOCRProcessor(image_size=image_size, base_size=base_size)
        batch_inputs = []
        for img in images:
            image_features = proc.tokenize_with_images(
                images=[img], bos=True, eos=True, cropping=crop_mode
            )
            cache_item = {
                "prompt": prompt,
                "multi_modal_data": {"image": image_features},
            }
            batch_inputs.append(cache_item)

        logits_processors = [
            NoRepeatNGramLogitsProcessor(
                ngram_size=40, window_size=90, whitelist_token_ids={128821, 128822}
            )
        ]

        sampling_params = SamplingParams(
            temperature=0.0,
            max_tokens=max_tokens,
            logits_processors=logits_processors,
            skip_special_tokens=False,
        )

        outputs_list = llm_local.generate(batch_inputs, sampling_params=sampling_params)

        ts = time.strftime("%Y%m%d_%H%M%S")
        out_dir = os.path.join("outputs", "vllm_gradio_batch", ts)
        os.makedirs(out_dir, exist_ok=True)

        preview_texts = []
        for idx, (output, file_path) in enumerate(zip(outputs_list, valid_paths)):
            content = output.outputs[0].text
            base_name = os.path.basename(file_path)
            name_no_ext = os.path.splitext(base_name)[0]
            # é¿å…æ–‡ä»¶åå†²çªï¼Œæ·»åŠ åºå·
            safe_name = f"{idx+1:03d}_{name_no_ext}"

            mmd_det_path = os.path.join(out_dir, f"{safe_name}_det.md")
            with open(mmd_det_path, "w", encoding="utf-8") as afile:
                afile.write(content)

            content_clean = clean_formula(content)
            matches_ref, mathes_other = re_match(content_clean)
            for a_match_other in mathes_other:
                content_clean = (
                    content_clean.replace(a_match_other, "")
                    .replace("\\n\\n\\n\\n", "\\n\\n")
                    .replace("\\n\\n\\n", "\\n\\n")
                    .replace("<center>", "")
                    .replace("</center>", "")
                )

            mmd_path = os.path.join(out_dir, f"{safe_name}.md")
            with open(mmd_path, "w", encoding="utf-8") as afile:
                afile.write(content_clean)

            if len(preview_texts) < 3:
                preview_texts.append(f"## {base_name}\n\n" + content_clean[:2000])

        # åˆ›å»º zip æ–‡ä»¶ä¾›ä¸‹è½½
        zip_path = os.path.join("outputs", "vllm_gradio_batch", f"batch_result_{ts}.zip")
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            for root, dirs, files in os.walk(out_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, out_dir)
                    zf.write(file_path, arcname)

        return f"å·²å¤„ç† {len(images)} å¼ å›¾ç‰‡ï¼Œç»“æœä¿å­˜åˆ°: {out_dir}", "\n\n".join(preview_texts), zip_path

    except Exception as e:
        import traceback
        return f"å¤„ç†å‡ºé”™: {str(e)}\n{traceback.format_exc()}", "", None


def process_batch(
    dir_path: str,
    prompt_type: str,
    custom_prompt: str,
    model_size: str,
    crop_mode: bool,
    max_concurrency: int,
    gpu_memory_utilization: float,
    max_tokens: int,
):
    """å¤„ç†æœåŠ¡å™¨æœ¬åœ°ç›®å½•ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰"""
    try:
        llm_local = init_llm(
            max_concurrency=max_concurrency,
            gpu_memory_utilization=gpu_memory_utilization,
            max_model_len=8192,
        )

        # ä½¿ç”¨å…¼å®¹ä¸­æ–‡è·¯å¾„çš„æ–¹æ³•åˆ—å‡ºå›¾ç‰‡
        images_path = _list_images_in_dir(dir_path)
        if not images_path:
            return f"ç›®å½•ä¸­æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ï¼ˆæ”¯æŒ jpg/png/webp/bmp/tiffï¼‰ï¼š{dir_path}", ""

        images = []
        for image_path in images_path:
            try:
                image = Image.open(image_path).convert("RGB")
                images.append(image)
            except Exception as e:
                print(f"skip file: {image_path} due to error: {e}")

        if prompt_type == "Free OCR":
            prompt = "<image>\nFree OCR. "
        elif prompt_type == "Markdown Conversion":
            prompt = "<image>\n<|grounding|>Convert the document to markdown. "
        elif prompt_type == "Custom":
            prompt = f"<image>\n{custom_prompt}"
        else:
            prompt = "<image>\nFree OCR. "

        preset = size_configs.get(model_size, size_configs["Gundam (Recommended)"])
        base_size = preset["base_size"]
        image_size = preset["image_size"]
        proc = DeepseekOCRProcessor(image_size=image_size, base_size=base_size)
        batch_inputs = []
        for img in images:
            image_features = proc.tokenize_with_images(
                images=[img], bos=True, eos=True, cropping=crop_mode
            )
            cache_item = {
                "prompt": prompt,
                "multi_modal_data": {"image": image_features},
            }
            batch_inputs.append(cache_item)

        logits_processors = [
            NoRepeatNGramLogitsProcessor(
                ngram_size=40, window_size=90, whitelist_token_ids={128821, 128822}
            )
        ]

        sampling_params = SamplingParams(
            temperature=0.0,
            max_tokens=max_tokens,
            logits_processors=logits_processors,
            skip_special_tokens=False,
        )

        outputs_list = llm_local.generate(batch_inputs, sampling_params=sampling_params)

        ts = time.strftime("%Y%m%d_%H%M%S")
        out_dir = os.path.join("outputs", "vllm_gradio_batch", ts)
        os.makedirs(out_dir, exist_ok=True)

        preview_texts = []
        for output, image_path in zip(outputs_list, images_path):
            content = output.outputs[0].text
            base_name = os.path.basename(image_path)
            name_no_ext = os.path.splitext(base_name)[0]

            mmd_det_path = os.path.join(out_dir, f"{name_no_ext}_det.md")
            with open(mmd_det_path, "w", encoding="utf-8") as afile:
                afile.write(content)

            content_clean = clean_formula(content)
            matches_ref, mathes_other = re_match(content_clean)
            for a_match_other in mathes_other:
                content_clean = (
                    content_clean.replace(a_match_other, "")
                    .replace("\\n\\n\\n\\n", "\\n\\n")
                    .replace("\\n\\n\\n", "\\n\\n")
                    .replace("<center>", "")
                    .replace("</center>", "")
                )

            mmd_path = os.path.join(out_dir, f"{name_no_ext}.md")
            with open(mmd_path, "w", encoding="utf-8") as afile:
                afile.write(content_clean)

            if len(preview_texts) < 3:
                preview_texts.append(f"## {base_name}\n\n" + content_clean[:2000])

        return f"å·²å†™å…¥ {len(images_path)} ä¸ªç»“æœåˆ°: {out_dir}", "\n\n".join(preview_texts)

    except Exception as e:
        import traceback
        return f"Error: {str(e)}\n\nTraceback:\n{traceback.format_exc()}", ""

def pdf_to_images_high_quality(pdf_path: str, dpi: int = 144, image_format: str = "PNG") -> List[Image.Image]:
    import fitz
    images: List[Image.Image] = []
    pdf_document = fitz.open(pdf_path)
    zoom = dpi / 72.0
    matrix = fitz.Matrix(zoom, zoom)
    for page_num in range(pdf_document.page_count):
        page = pdf_document[page_num]
        pixmap = page.get_pixmap(matrix=matrix, alpha=False)
        Image.MAX_IMAGE_PIXELS = None
        img_data = pixmap.tobytes("png")
        img = Image.open(io.BytesIO(img_data))
        if img.mode in ("RGBA", "LA"):
            background = Image.new("RGB", img.size, (255, 255, 255))
            background.paste(img, mask=img.split()[-1] if img.mode == "RGBA" else None)
            img = background
        images.append(img)
    pdf_document.close()
    return images

def pil_to_pdf_img2pdf(pil_images: List[Image.Image], output_path: str):
    import img2pdf
    if not pil_images:
        return
    image_bytes_list = []
    for img in pil_images:
        if img.mode != "RGB":
            img = img.convert("RGB")
        img_buffer = io.BytesIO()
        img.save(img_buffer, format="JPEG", quality=95)
        img_bytes = img_buffer.getvalue()
        image_bytes_list.append(img_bytes)
    try:
        pdf_bytes = img2pdf.convert(image_bytes_list)
        with open(output_path, "wb") as f:
            f.write(pdf_bytes)
    except Exception as e:
        print(f"error: {e}")

def extract_coordinates_and_label(ref_text: Tuple[str, str, str], image_width: int, image_height: int):
    try:
        label_type = ref_text[1]
        cor_list = eval(ref_text[2])
    except Exception as e:
        print(e)
        return None
    return (label_type, cor_list)

def draw_bounding_boxes(image: Image.Image, refs: List[Tuple[str, str, str]], jdx: int, save_dir: str):
    image_width, image_height = image.size
    img_draw = image.copy()
    draw = ImageDraw.Draw(img_draw)
    overlay = Image.new("RGBA", img_draw.size, (0, 0, 0, 0))
    draw2 = ImageDraw.Draw(overlay)
    font = ImageFont.load_default()
    img_idx = 0
    os.makedirs(os.path.join(save_dir, "images"), exist_ok=True)
    for i, ref in enumerate(refs):
        try:
            result = extract_coordinates_and_label(ref, image_width, image_height)
            if result:
                label_type, points_list = result
                color = (
                    np.random.randint(0, 200),
                    np.random.randint(0, 200),
                    np.random.randint(0, 255),
                )
                color_a = color + (20,)
                for points in points_list:
                    x1, y1, x2, y2 = points
                    x1 = int(x1 / 999 * image_width)
                    y1 = int(y1 / 999 * image_height)
                    x2 = int(x2 / 999 * image_width)
                    y2 = int(y2 / 999 * image_height)
                    if label_type == "image":
                        try:
                            cropped = image.crop((x1, y1, x2, y2))
                            cropped.save(os.path.join(save_dir, "images", f"{jdx}_{img_idx}.jpg"))
                        except Exception as e:
                            print(e)
                        img_idx += 1
                    try:
                        if label_type == "title":
                            draw.rectangle([x1, y1, x2, y2], outline=color, width=4)
                            draw2.rectangle(
                                [x1, y1, x2, y2], fill=color_a, outline=(0, 0, 0, 0), width=1
                            )
                        else:
                            draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
                            draw2.rectangle(
                                [x1, y1, x2, y2], fill=color_a, outline=(0, 0, 0, 0), width=1
                            )
                        text_x = x1
                        text_y = max(0, y1 - 15)
                        text_bbox = draw.textbbox((0, 0), label_type, font=font)
                        text_width = text_bbox[2] - text_bbox[0]
                        text_height = text_bbox[3] - text_bbox[1]
                        draw.rectangle(
                            [text_x, text_y, text_x + text_width, text_y + text_height],
                            fill=(255, 255, 255, 30),
                        )
                        draw.text((text_x, text_y), label_type, font=font, fill=color)
                    except Exception:
                        pass
        except Exception:
            continue
    img_draw.paste(overlay, (0, 0), overlay)
    return img_draw

def process_pdf(
    pdf_path: str,
    dpi: int,
    prompt_type: str,
    custom_prompt: str,
    model_size: str,
    crop_mode: bool,
    max_concurrency: int,
    gpu_memory_utilization: float,
    max_tokens: int,
    export_layout_pdf: bool,
):
    try:
        # Guard empty input
        if not pdf_path:
            return "æœªæ£€æµ‹åˆ° PDF æ–‡ä»¶ï¼Œè¯·å…ˆä¸Šä¼ åå†ç‚¹å‡»å¤„ç†ã€‚", "", None
        if isinstance(pdf_path, str) and not os.path.exists(pdf_path):
            return f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{pdf_path}", "", None
        llm_local = init_llm(
            max_concurrency=max_concurrency,
            gpu_memory_utilization=gpu_memory_utilization,
            max_model_len=8192,
        )

        if prompt_type == "Free OCR":
            prompt = "<image>\nFree OCR. "
        elif prompt_type == "Markdown Conversion":
            prompt = "<image>\n<|grounding|>Convert the document to markdown. "
        elif prompt_type == "Custom":
            prompt = f"<image>\n{custom_prompt}"
        else:
            prompt = "<image>\nFree OCR. "

        images = pdf_to_images_high_quality(pdf_path, dpi=dpi)
        if not images:
            return "PDF ä¸­æ— å¯å¤„ç†é¡µé¢", "", None

        preset = size_configs.get(model_size, size_configs["Gundam (Recommended)"])
        base_size = preset["base_size"]
        image_size = preset["image_size"]
        proc = DeepseekOCRProcessor(image_size=image_size, base_size=base_size)
        batch_inputs = []
        for img in images:
            image_features = proc.tokenize_with_images(
                images=[img], bos=True, eos=True, cropping=crop_mode
            )
            cache_item = {
                "prompt": prompt,
                "multi_modal_data": {"image": image_features},
            }
            batch_inputs.append(cache_item)

        logits_processors = [
            NoRepeatNGramLogitsProcessor(
                ngram_size=20, window_size=50, whitelist_token_ids={128821, 128822}
            )
        ]
        sampling_params = SamplingParams(
            temperature=0.0,
            max_tokens=max_tokens,
            logits_processors=logits_processors,
            skip_special_tokens=False,
            include_stop_str_in_output=True,
        )

        outputs_list = llm_local.generate(batch_inputs, sampling_params=sampling_params)

        ts = time.strftime("%Y%m%d_%H%M%S")
        out_dir = os.path.join("outputs", "vllm_gradio_pdf", ts)
        os.makedirs(out_dir, exist_ok=True)
        os.makedirs(os.path.join(out_dir, "images"), exist_ok=True)

        contents_det = ""
        contents = ""
        draw_images: List[Image.Image] = []

        jdx = 0
        for output, img in zip(outputs_list, images):
            content = output.outputs[0].text
            if "<ï½œendâ–ofâ–sentenceï½œ>" in content:
                content = content.replace("<ï½œendâ–ofâ–sentenceï½œ>", "")

            page_num = f"\n<--- Page Split --->"
            contents_det += content + f"\n{page_num}\n"

            image_draw = img.copy()
            matches_ref, matches_images, _ = re_match_pdf(content)
            result_image = draw_bounding_boxes(image_draw, matches_ref, jdx, out_dir)
            draw_images.append(result_image)

            for idx, a_match in enumerate(matches_images):
                content = content.replace(
                    a_match,
                    f"![](images/" + str(jdx) + "_" + str(idx) + ".jpg)\n",
                )
            content = (
                content.replace("\\coloneqq", ":=")
                .replace("\\eqqcolon", "=:")
                .replace("\n\n\n\n", "\n\n")
                .replace("\n\n\n", "\n\n")
            )

            contents += content + f"\n{page_num}\n"
            jdx += 1

        base_name = os.path.basename(pdf_path)
        mmd_det_path = os.path.join(out_dir, base_name.replace(".pdf", "_det.mmd"))
        mmd_path = os.path.join(out_dir, base_name.replace(".pdf", ".mmd"))
        pdf_out_path = os.path.join(out_dir, base_name.replace(".pdf", "_layouts.pdf"))

        with open(mmd_det_path, "w", encoding="utf-8") as afile:
            afile.write(contents_det)
        with open(mmd_path, "w", encoding="utf-8") as afile:
            afile.write(contents)

        # åˆ›å»º zip æ–‡ä»¶ä¾›ä¸‹è½½
        zip_path = os.path.join("outputs", "vllm_gradio_pdf", f"pdf_result_{ts}.zip")
        
        if export_layout_pdf:
            pil_to_pdf_img2pdf(draw_images, pdf_out_path)
        
        # æ‰“åŒ…æ‰€æœ‰ç»“æœåˆ° zip
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            for root, dirs, files in os.walk(out_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, out_dir)
                    zf.write(file_path, arcname)
        
        return contents, contents_det, zip_path

    except Exception as e:
        import traceback
        return f"Error: {str(e)}\n\nTraceback:\n{traceback.format_exc()}", "", None


def create_demo():
    # Older Gradio versions may not support the theme kwarg; keep compatibility by omitting it.
    with gr.Blocks(title="DeepSeek-OCR vLLM Demo") as demo:
        gr.Markdown(
            """
            > ğŸ›ˆ å¼•æ“é‡å»º vs æœåŠ¡é‡å¯
            - ç‚¹å‡»â€œâ™»ï¸ é‡å¯å¼•æ“â€ä»…æ¸…ç†æ˜¾å­˜å¹¶é‡å»º vLLM å¼•æ“ï¼›ä¸ä¼šåº”ç”¨ä»£ç æ”¹åŠ¨
            - ä¿®æ”¹ Python æºç æˆ– UI å¸ƒå±€åè¯·ä½¿ç”¨ `run_demo.sh` é‡å¯æœåŠ¡
            - é‡åˆ° CUDA/æ˜¾å­˜å¼‚å¸¸ï¼šå…ˆå°è¯•â€œé‡å¯å¼•æ“â€ï¼Œä»å¼‚å¸¸å†é‡å¯æœåŠ¡
            - éœ€è¦è‡ªåŠ¨é‡å¯æœåŠ¡ï¼šä½¿ç”¨ `./run_demo.sh --watch` å¯ç”¨æ–‡ä»¶ç›‘å¬
            """
        )
        gr.Markdown(
            """
            # ğŸ” DeepSeek-OCR vLLM Demo

            ä½¿ç”¨ vLLM å¼•æ“è¿›è¡Œç¦»çº¿ OCR æ¨ç†ï¼ˆæœ¬åœ° MODEL/TOKENIZERï¼‰ã€‚
            - æ”¯æŒå¹¶å‘ä¸æ˜¾å­˜å‚æ•°é…ç½®
            - é€‚é… 4090Dï¼ŒKV Cache é«˜å¹¶å‘
            - æä¾› å•å›¾ / æ‰¹é‡ / PDF ä¸‰ç§æ¨¡å¼
            """
        )

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ è®¾ç½®ï¼ˆé€šç”¨ï¼‰")
                prompt_type = gr.Radio(
                    choices=["Free OCR", "Markdown Conversion", "Custom"],
                    value="Markdown Conversion",
                    label="Prompt ç±»å‹",
                )
                custom_prompt = gr.Textbox(
                    label="è‡ªå®šä¹‰ Promptï¼ˆé€‰æ‹© Custom æ—¶ç”Ÿæ•ˆï¼‰",
                    placeholder="è¾“å…¥è‡ªå®šä¹‰æŒ‡ä»¤...",
                    lines=2,
                    visible=False,
                )
                crop_mode = gr.Checkbox(
                    label="å¯ç”¨è£å‰ªï¼ˆCROP_MODEï¼‰",
                    value=bool(CROP_MODE),
                )
                model_size = gr.Radio(
                    choices=[
                        "Tiny",
                        "Small",
                        "Base",
                        "Large",
                        "Gundam (Recommended)",
                    ],
                    value="Base",
                    label="æ¨¡å‹å°ºå¯¸é¢„è®¾",
                )
                with gr.Accordion("é«˜çº§å‚æ•°", open=False):
                    # åŠ¨æ€è§£æå¹¶å‘æ»‘æ¡çš„é»˜è®¤å€¼ä¸ä¸Šé™ï¼Œé¿å…é»˜è®¤å€¼è¶Šç•Œ
                    try:
                        _default_concurrency = int(MAX_CONCURRENCY) if MAX_CONCURRENCY else 12
                    except Exception:
                        _default_concurrency = 12
                    _concurrency_max = max(16, _default_concurrency)
                    _concurrency_default = min(_default_concurrency, _concurrency_max)
                    max_concurrency = gr.Slider(
                        minimum=1,
                        maximum=_concurrency_max,
                        step=1,
                        value=_concurrency_default,
                        label="å¹¶å‘ï¼ˆmax_num_seqsï¼‰",
                    )
                    gpu_memory_utilization = gr.Slider(
                        minimum=0.5,
                        maximum=0.98,
                        step=0.01,
                        value=0.85,
                        label="æ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆgpu_memory_utilizationï¼‰",
                    )
                    max_tokens = gr.Slider(
                        minimum=256,
                        maximum=16384,
                        step=512,
                        value=16384,
                        label="ç”Ÿæˆé•¿åº¦ï¼ˆmax_tokensï¼‰",
                    )
                    restart_btn = gr.Button("â™»ï¸ é‡å¯å¼•æ“ï¼ˆæ¸…ç†æ˜¾å­˜ï¼‰")
                    restart_service_btn = gr.Button("ğŸ”„ é‡å¯æœåŠ¡ï¼ˆwatchï¼‰")
                    engine_status = gr.Textbox(
                        label="å¼•æ“çŠ¶æ€",
                        value="å·²å°±ç»ª",
                        lines=2,
                        interactive=False,
                    )
                    estimate_btn = gr.Button("ğŸ§® æ ¹æ®æ˜¾å­˜ä¼°ç®—å¹¶å‘")

            with gr.Column(scale=1):
                gr.Markdown(
                    f"""
                    ### ğŸ”§ å½“å‰æ¨¡å‹
                    - MODEL_PATH: `{MODEL_PATH}`
                    - TOKENIZER_PATH: `{TOKENIZER_PATH}`
                    """
                )

        with gr.Tabs():
            with gr.Tab("å•å›¾"):
                with gr.Row():
                    with gr.Column(scale=1):
                        image_input = gr.Image(
                            label="ä¸Šä¼ å›¾ç‰‡",
                            type="pil",
                            sources=["upload", "clipboard"],
                        )
                        process_btn_single = gr.Button("ğŸš€ å¼€å§‹å¤„ç†ï¼ˆå•å›¾ï¼‰", variant="primary")
                    with gr.Column(scale=1):
                        output_text_single = gr.Textbox(
                            label="æå–æ–‡æœ¬",
                            lines=20,
                            max_lines=30,
                        )

            with gr.Tab("æ‰¹é‡"):
                gr.Markdown("**ä¸Šä¼ å¤šå¼ å›¾ç‰‡è¿›è¡Œæ‰¹é‡ OCR å¤„ç†**ï¼ˆæ”¯æŒ jpg/png/webp/bmp/tiffï¼‰")
                with gr.Row():
                    with gr.Column(scale=1):
                        batch_files = gr.File(
                            label="ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯å¤šé€‰ï¼‰",
                            file_count="multiple",
                            file_types=["image"],
                            type="filepath",
                        )
                        process_btn_batch = gr.Button("ğŸš€ å¼€å§‹å¤„ç†ï¼ˆæ‰¹é‡ï¼‰", variant="primary")
                    with gr.Column(scale=1):
                        batch_outdir_text = gr.Textbox(
                            label="å¤„ç†çŠ¶æ€",
                            lines=2,
                            interactive=False,
                        )
                        batch_download = gr.File(
                            label="ğŸ“¥ ä¸‹è½½ç»“æœï¼ˆZIPï¼‰",
                            interactive=False,
                        )
                        batch_preview_text = gr.Textbox(
                            label="é¢„è§ˆï¼ˆå‰3é¡¹èŠ‚é€‰ï¼‰",
                            lines=15,
                            max_lines=25,
                        )

            with gr.Tab("PDF"):
                with gr.Row():
                    with gr.Column(scale=1):
                        pdf_file = gr.File(
                            label="ä¸Šä¼  PDF",
                            file_types=[".pdf"],
                            type="filepath",
                        )
                        pdf_dpi = gr.Slider(
                            minimum=72,
                            maximum=288,
                            step=12,
                            value=144,
                            label="PDF DPIï¼ˆæ¸²æŸ“ï¼‰",
                        )
                        export_layout_pdf = gr.Checkbox(
                            label="å¯¼å‡ºå¸ƒå±€ PDFï¼ˆè¾ƒæ…¢ï¼Œé»˜è®¤å…³é—­ï¼‰",
                            value=False,
                        )
                        process_btn_pdf = gr.Button("ğŸš€ å¼€å§‹å¤„ç†ï¼ˆPDFï¼‰", variant="primary")
                    with gr.Column(scale=1):
                        pdf_mmd_text = gr.Textbox(
                            label="Markdown è¾“å‡ºï¼ˆåˆå¹¶ï¼‰",
                            lines=20,
                            max_lines=30,
                        )
                        pdf_det_text = gr.Textbox(
                            label="æ£€æµ‹è¾“å‡ºï¼ˆåˆå¹¶ï¼‰",
                            lines=20,
                            max_lines=30,
                        )
                        pdf_layouts_file = gr.File(
                            label="ğŸ“¥ ä¸‹è½½ç»“æœï¼ˆZIPï¼Œå« Markdown + å›¾ç‰‡ï¼‰",
                            interactive=False,
                        )

        def update_prompt_visibility(choice):
            return gr.update(visible=(choice == "Custom"))

        prompt_type.change(
            fn=update_prompt_visibility,
            inputs=[prompt_type],
            outputs=[custom_prompt],
        )

        # å½“é€‰æ‹©å°ºå¯¸é¢„è®¾æ—¶ï¼Œæ›´æ–°è£å‰ªæ¨èå€¼
        def apply_size_preset(choice):
            preset = size_configs.get(choice, size_configs["Gundam (Recommended)"])
            return gr.update(value=preset["crop_mode"]) 

        model_size.change(
            fn=apply_size_preset,
            inputs=[model_size],
            outputs=[crop_mode],
        )

        # è§¦å‘æœåŠ¡çº§é‡å¯ï¼ˆwatch æ¨¡å¼ä¸‹é€šè¿‡å†™å…¥ä¿¡å·æ–‡ä»¶è§¦å‘ï¼‰
        def trigger_service_restart():
            try:
                sig_path = os.path.join(ROOT_DIR, "watch_restart.json")
                import json
                ts = time.time()
                with open(sig_path, "w", encoding="utf-8") as f:
                    json.dump({"restart_at": ts}, f)
                return "å·²è§¦å‘æœåŠ¡é‡å¯ï¼ˆwatchï¼‰ï¼Œè¯·ç¨ååˆ·æ–°é¡µé¢ã€‚"
            except Exception as e:
                return f"è§¦å‘å¤±è´¥ï¼š{e}"

        # åŸºäº GPU æ˜¾å­˜ä¸ max_tokens çš„å¹¶å‘ä¼°ç®—
        def estimate_concurrency_action(gmu: float, max_toks: int):
            try:
                if torch.cuda.is_available():
                    free_b, total_b = torch.cuda.mem_get_info()
                    total_gb = total_b / (1024 ** 3)
                    free_gb = free_b / (1024 ** 3)
                    # æœ‰æ•ˆå¯ç”¨æ˜¾å­˜ï¼šè€ƒè™‘ slider çš„ gmuï¼Œå°½é‡ä¸è¶…å½“å‰ç©ºé—²
                    effective_gb = max(min(total_gb * gmu, free_gb) - 1.0, 1.0)
                else:
                    effective_gb = 8.0
            except Exception:
                try:
                    props = torch.cuda.get_device_properties(0)
                    total_gb = props.total_memory / (1024 ** 3)
                    effective_gb = max(total_gb * gmu - 1.0, 1.0)
                except Exception:
                    effective_gb = 8.0

            # ç»éªŒä¼°ç®—ï¼š8192 tokens æ—¶æ¯å¹¶å‘çº¦ ~800MBï¼›çº¿æ€§éš max_tokens å˜åŒ–
            per_seq_mb = 800.0 * max(1.0, float(max_toks) / 8192.0)
            est = int(max(1, (effective_gb * 1024.0) / per_seq_mb))
            try:
                cfg_max = int(MAX_CONCURRENCY) if MAX_CONCURRENCY else 16
            except Exception:
                cfg_max = 16
            new_max = max(16, cfg_max, est)
            est = min(est, new_max)
            return gr.update(value=est, maximum=new_max)

        estimate_btn.click(
            fn=estimate_concurrency_action,
            inputs=[gpu_memory_utilization, max_tokens],
            outputs=[max_concurrency],
        )

        # å¼ºåˆ¶é‡å¯å¼•æ“ï¼ˆæ¸…ç†æ˜¾å­˜å¹¶æŒ‰å½“å‰å‚æ•°é‡å»ºï¼‰
        def restart_engine_action(max_conc: int, gmu: float):
            try:
                _ensure_offline_env()
                _setup_cuda_env()
                # force_reinit è§¦å‘æ¸…ç†é€»è¾‘
                _ = init_llm(
                    max_concurrency=max_conc,
                    gpu_memory_utilization=gmu,
                    max_model_len=8192,
                    force_reinit=True,
                )
                return "å¼•æ“å·²é‡å¯ï¼šå¹¶å‘=%dï¼Œæ˜¾å­˜åˆ©ç”¨ç‡=%.2fï¼Œmax_len=8192" % (max_conc, gmu)
            except Exception as e:
                import traceback
                return f"é‡å¯å¤±è´¥ï¼š{str(e)}\n\n{traceback.format_exc()}"

        restart_btn.click(
            fn=restart_engine_action,
            inputs=[max_concurrency, gpu_memory_utilization],
            outputs=[engine_status],
        )

        restart_service_btn.click(
            fn=trigger_service_restart,
            inputs=[],
            outputs=[engine_status],
        )

        process_btn_single.click(
            fn=process_image,
            inputs=[
                image_input,
                prompt_type,
                custom_prompt,
                model_size,
                crop_mode,
                max_concurrency,
                gpu_memory_utilization,
                max_tokens,
            ],
            outputs=[output_text_single],
        )

        process_btn_batch.click(
            fn=process_batch_upload,
            inputs=[
                batch_files,
                prompt_type,
                custom_prompt,
                model_size,
                crop_mode,
                max_concurrency,
                gpu_memory_utilization,
                max_tokens,
            ],
            outputs=[batch_outdir_text, batch_preview_text, batch_download],
        )

        process_btn_pdf.click(
            fn=process_pdf,
            inputs=[
                pdf_file,
                pdf_dpi,
                prompt_type,
                custom_prompt,
                model_size,
                crop_mode,
                max_concurrency,
                gpu_memory_utilization,
                max_tokens,
                export_layout_pdf,
            ],
            outputs=[pdf_mmd_text, pdf_det_text, pdf_layouts_file],
        )

    return demo


if __name__ == "__main__":
    _ensure_offline_env()
    _setup_cuda_env()
    # å¯é€‰å¯åŠ¨é¢„çƒ­ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ WARMUP_ON_START=1 å¯ç”¨
    def warmup_engine_on_start():
        try:
            if os.environ.get("WARMUP_ON_START", "0") != "1":
                return
            try:
                _default_concurrency = int(MAX_CONCURRENCY) if MAX_CONCURRENCY else 8
            except Exception:
                _default_concurrency = 8
            gmu = 0.85
            llm_local = init_llm(
                max_concurrency=_default_concurrency,
                gpu_memory_utilization=gmu,
                max_model_len=8192,
            )
            # æ„é€ æå°å›¾åƒè¿›è¡Œä¸€æ¬¡è½»é‡ç”Ÿæˆä»¥è§¦å‘å›¾æ•è·ä¸ç¼“å­˜
            from PIL import Image as _Image
            img = _Image.new("RGB", (64, 64), color=(255, 255, 255))
            proc = DeepseekOCRProcessor(image_size=640, base_size=1024)
            image_features = proc.tokenize_with_images(images=[img], bos=True, eos=True, cropping=False)
            cache_item = {"prompt": "<image>\nWarmup.", "multi_modal_data": {"image": image_features}}
            sp = SamplingParams(temperature=0.0, max_tokens=16, skip_special_tokens=True)
            llm_local.generate([cache_item], sampling_params=sp)
        except Exception:
            # é¢„çƒ­å¤±è´¥ä¸å½±å“æ­£å¸¸å¯åŠ¨
            pass

    warmup_engine_on_start()
    demo = create_demo()
    port = int(os.environ.get("DEMO_PORT", "7860"))
    demo.launch(
        server_name="0.0.0.0",
        server_port=port,
        share=False,
        debug=True,
    )
