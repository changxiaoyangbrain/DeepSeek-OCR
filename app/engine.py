"""
æ¨ç†å¼•æ“æ¨¡å— - LLM åˆå§‹åŒ–å’Œç®¡ç†
"""
import time
import gc
import torch
from typing import Optional

from vllm import LLM, SamplingParams
from vllm.model_executor.models.registry import ModelRegistry

from .config import MODEL_PATH, TOKENIZER_PATH, ensure_offline_env, setup_cuda_env
from .utils import log_info, log_success, log_warning

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹
from deepseek_ocr import DeepseekOCRForCausalLM

# å…¨å±€ LLM å®ä¾‹
_llm: Optional[LLM] = None
_current_engine_cfg = {
    "max_concurrency": None,
    "gpu_memory_utilization": None,
    "max_model_len": None,
}


def get_llm() -> Optional[LLM]:
    """è·å–å½“å‰ LLM å®ä¾‹"""
    global _llm
    return _llm


def init_llm(
    max_concurrency: int,
    gpu_memory_utilization: float,
    max_model_len: int = 8192,
    force_reinit: bool = False,
) -> LLM:
    """
    åˆå§‹åŒ–æˆ–å¤ç”¨ LLM å¼•æ“
    """
    global _llm, _current_engine_cfg

    if (
        _llm is not None
        and not force_reinit
        and _current_engine_cfg.get("max_concurrency") == max_concurrency
        and _current_engine_cfg.get("gpu_memory_utilization") == gpu_memory_utilization
        and _current_engine_cfg.get("max_model_len") == max_model_len
    ):
        return _llm

    # æ¸…ç†æ—§å¼•æ“
    if _llm is not None or force_reinit:
        log_info("ğŸ”„ æ­£åœ¨æ¸…ç†æ—§å¼•æ“...")
        try:
            del _llm
        except Exception:
            pass
        _llm = None
        try:
            torch.cuda.empty_cache()
        except Exception:
            pass
        try:
            if hasattr(torch.cuda, "ipc_collect"):
                torch.cuda.ipc_collect()
        except Exception:
            pass
        gc.collect()
        time.sleep(0.3)

    ensure_offline_env()
    setup_cuda_env()

    # æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹
    if "DeepseekOCRForCausalLM" not in ModelRegistry.get_supported_archs():
        ModelRegistry.register_model(
            "DeepseekOCRForCausalLM", DeepseekOCRForCausalLM
        )

    log_info(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ¨ç†å¼•æ“...")
    log_info(f"   æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    log_info(f"   å¹¶å‘æ•°: {max_concurrency}")
    log_info(f"   æ˜¾å­˜åˆ©ç”¨ç‡: {gpu_memory_utilization:.0%}")
    log_info(f"   æœ€å¤§åºåˆ—é•¿åº¦: {max_model_len}")

    try:
        _llm = LLM(
            model=MODEL_PATH,
            tokenizer=TOKENIZER_PATH,
            hf_overrides={"architectures": ["DeepseekOCRForCausalLM"]},
            block_size=256,
            enforce_eager=False,
            trust_remote_code=False,
            max_model_len=max_model_len,
            swap_space=0,
            max_num_seqs=max_concurrency,
            tensor_parallel_size=1,
            gpu_memory_utilization=gpu_memory_utilization,
            disable_mm_preprocessor_cache=True,
        )
    except AssertionError:
        # é‡è¯•ä¸€æ¬¡
        log_warning("é¦–æ¬¡åˆå§‹åŒ–å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•...")
        torch.cuda.empty_cache()
        gc.collect()
        time.sleep(0.5)
        _llm = LLM(
            model=MODEL_PATH,
            tokenizer=TOKENIZER_PATH,
            hf_overrides={"architectures": ["DeepseekOCRForCausalLM"]},
            block_size=256,
            enforce_eager=False,
            trust_remote_code=False,
            max_model_len=max_model_len,
            swap_space=0,
            max_num_seqs=max_concurrency,
            tensor_parallel_size=1,
            gpu_memory_utilization=gpu_memory_utilization,
            disable_mm_preprocessor_cache=True,
        )

    _current_engine_cfg = {
        "max_concurrency": max_concurrency,
        "gpu_memory_utilization": gpu_memory_utilization,
        "max_model_len": max_model_len,
    }
    
    log_success("âœ… æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    return _llm


def restart_engine(max_concurrency: int, gpu_memory_utilization: float) -> str:
    """å¼ºåˆ¶é‡å¯å¼•æ“"""
    try:
        ensure_offline_env()
        setup_cuda_env()
        init_llm(
            max_concurrency=max_concurrency,
            gpu_memory_utilization=gpu_memory_utilization,
            max_model_len=8192,
            force_reinit=True,
        )
        return "âœ… å¼•æ“é‡å¯æˆåŠŸ"
    except Exception as e:
        return f"âŒ å¼•æ“é‡å¯å¤±è´¥: {e}"


def warmup_engine(
    max_concurrency: int = 12,
    gpu_memory_utilization: float = 0.85,
    max_model_len: int = 8192,
) -> bool:
    """
    é¢„çƒ­æ¨¡å‹å¼•æ“ - å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹åˆ°æ˜¾å­˜ï¼Œå‡å°‘ç”¨æˆ·é¦–æ¬¡è¯·æ±‚ç­‰å¾…æ—¶é—´
    è¿”å›: True è¡¨ç¤ºé¢„çƒ­æˆåŠŸï¼ŒFalse è¡¨ç¤ºè·³è¿‡æˆ–å¤±è´¥
    """
    import os
    from PIL import Image
    import numpy as np
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨é¢„çƒ­
    warmup_flag = os.environ.get("WARMUP_ON_START", "1")
    if warmup_flag not in ("1", "true", "True", "yes", "YES"):
        log_info("â­ï¸  é¢„çƒ­å·²ç¦ç”¨ (WARMUP_ON_START != 1)")
        return False
    
    log_info("=" * 60)
    log_info("ğŸ”¥ å¼€å§‹æ¨¡å‹é¢„çƒ­...")
    log_info("=" * 60)
    
    warmup_start = time.time()
    
    try:
        # 1. åˆå§‹åŒ–å¼•æ“ï¼ˆåŠ è½½æ¨¡å‹åˆ°æ˜¾å­˜ï¼‰
        log_info("ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°æ˜¾å­˜...")
        llm = init_llm(
            max_concurrency=max_concurrency,
            gpu_memory_utilization=gpu_memory_utilization,
            max_model_len=max_model_len,
        )
        
        # 2. åˆ›å»ºæµ‹è¯•å›¾ç‰‡ï¼ˆå°å°ºå¯¸ç™½è‰²å›¾ç‰‡ï¼‰
        log_info("ğŸ–¼ï¸  æ­£åœ¨å‡†å¤‡é¢„çƒ­å›¾ç‰‡...")
        test_image = Image.new("RGB", (256, 256), color=(255, 255, 255))
        
        # 3. å¯¼å…¥å¤„ç†å™¨
        import sys
        ROOT_DIR = os.path.dirname(os.path.dirname(__file__))
        VLLM_DIR = os.path.join(ROOT_DIR, "DeepSeek-OCR-master", "DeepSeek-OCR-vllm")
        if VLLM_DIR not in sys.path:
            sys.path.append(VLLM_DIR)
        
        from process.image_process import DeepseekOCRProcessor
        from process.ngram_norepeat import NoRepeatNGramLogitsProcessor
        
        # 4. é¢„å¤„ç†æµ‹è¯•å›¾ç‰‡
        log_info("âš™ï¸  æ­£åœ¨é¢„å¤„ç†æµ‹è¯•å›¾ç‰‡...")
        proc = DeepseekOCRProcessor(image_size=640, base_size=1024)
        image_features = proc.tokenize_with_images(
            images=[test_image], bos=True, eos=True, cropping=False
        )
        
        # 5. æ‰§è¡Œä¸€æ¬¡æ¨ç†ï¼ˆçœŸæ­£çš„é¢„çƒ­ï¼‰
        log_info("ğŸš€ æ­£åœ¨æ‰§è¡Œé¢„çƒ­æ¨ç†...")
        
        logits_processors = [
            NoRepeatNGramLogitsProcessor(
                ngram_size=40, window_size=90, whitelist_token_ids={128821, 128822}
            )
        ]
        
        sampling_params = SamplingParams(
            temperature=0.0,
            max_tokens=64,  # é¢„çƒ­æ—¶åªç”Ÿæˆå°‘é‡ token
            logits_processors=logits_processors,
            skip_special_tokens=False,
        )
        
        warmup_prompt = "<|im_start|>User:<image>\nOCR this image<|im_end|>\n<|im_start|>Assistant:<ï½œdetâ–ofâ–imageï½œ>"
        
        cache_item = {
            "prompt": warmup_prompt,
            "multi_modal_data": {"image": image_features},
        }
        
        # æ‰§è¡Œé¢„çƒ­æ¨ç†
        _ = llm.generate([cache_item], sampling_params=sampling_params)
        
        warmup_time = time.time() - warmup_start
        
        # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            log_info(f"ğŸ’¾ æ˜¾å­˜ä½¿ç”¨: å·²åˆ†é… {allocated:.2f} GB / å·²é¢„ç•™ {reserved:.2f} GB")
        
        log_info("=" * 60)
        log_success(f"ğŸ”¥ æ¨¡å‹é¢„çƒ­å®Œæˆï¼è€—æ—¶ {warmup_time:.2f} ç§’")
        log_info("   âœ… æ¨¡å‹å·²åŠ è½½åˆ°æ˜¾å­˜ï¼Œç”¨æˆ·é¦–æ¬¡è¯·æ±‚å°†æ— éœ€ç­‰å¾…åŠ è½½")
        log_info("=" * 60)
        
        return True
        
    except Exception as e:
        log_warning(f"âš ï¸  æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
        log_warning("   æœåŠ¡å°†ç»§ç»­å¯åŠ¨ï¼Œä½†é¦–æ¬¡è¯·æ±‚å¯èƒ½éœ€è¦ç­‰å¾…æ¨¡å‹åŠ è½½")
        return False
