# 远程调用 DeepSeek-OCR（Gradio / HTTP）

本机已包含可对外监听的 Gradio 服务入口（`gradio_vllm_demo.py` 与 `gradio_demo.py` 默认 `server_name="0.0.0.0"`，端口 `7860`）。下面给出安全地从远程机器访问本机 DeepSeek-OCR 的推荐方案。

## 角色与最小步骤
- 服务器：运行 DeepSeek-OCR 的这台机器（本机）。
- 客户端：需要从远程调用 OCR 的机器。
- 最简单路径：服务器启动 demo → 客户端用 SSH 本地转发 → 客户端调用 `http://localhost:9000/run/predict`。

## 场景补充：Gradio 在同一局域网的另一台机器上
- 要求：那台机器同样具备 GPU 与本地模型/分词器缓存，已激活 `conda activate deepseek-ocr`。
- 启动时确保监听局域网地址（通常默认 `0.0.0.0` 已满足），示例：
```bash
export HF_HUB_OFFLINE=1 TRANSFORMERS_OFFLINE=1 HF_HUB_DISABLE_TELEMETRY=1 HF_HUB_ENABLE_HF_TRANSFER=1
export CUDA_VISIBLE_DEVICES=0 DEMO_PORT=7860
python -u gradio_vllm_demo.py  # 或 ./run_demo.sh
# 预期监听 http://<lan_ip>:7860
```
- 访问：局域网内其他主机直接打开 `http://<lan_ip>:7860/` 或用 HTTP 客户端指向该地址；将下文示例中的 `localhost` 替换为 `<lan_ip>`。
- 网络要求：关闭本机防火墙或放行 7860；若有交换机 ACL/网关隔离，需允许该端口。
- 安全建议：若局域网不完全可信，仍可用 SSH 隧道（方案 A/B）或在前面加反向代理（nginx/caddy）做 Basic Auth。

## 前置条件（服务器侧）
- 已在本机激活环境并可本地运行 demo：
```bash
conda activate deepseek-ocr
export HF_HUB_OFFLINE=1 TRANSFORMERS_OFFLINE=1 HF_HUB_DISABLE_TELEMETRY=1 HF_HUB_ENABLE_HF_TRANSFER=1
export CUDA_VISIBLE_DEVICES=0 DEMO_PORT=7860
python -u gradio_vllm_demo.py
# 或使用 ./run_demo.sh （支持 --watch 自动重启）
```
- 确认服务监听：`netstat -tlnp | grep 7860`，预期监听 `0.0.0.0:7860`。
- 服务器开启 SSH 登录（用于隧道），防火墙已放行 22 端口（或你自定义的 SSH 端口）。
- 获取服务器 IP：`hostname -I` 或 `ip a`；外网需结合云厂商安全组。

## 方案 A：SSH 本地转发（推荐，安全、无需改防火墙）
在远程机器执行，将本地 9000 端口转发到服务器 7860：
```bash
ssh -N -L 9000:localhost:7860 user@your_server_ip
```
访问远程机器浏览器/HTTP 客户端的 `http://localhost:9000/` 即可使用本机 Gradio 页面和 API。

## 方案 B：SSH 反向转发（服务器不能出网但客户端能连入服务器）
在服务器上执行，将服务器 7860 暴露到远端可达的跳板 `jump_host`：
```bash
ssh -N -R 9000:localhost:7860 user@jump_host
```
之后在 `jump_host` 或可连到 `jump_host` 的机器上访问 `http://localhost:9000/`。

## 方案 C：直接暴露 HTTP（不推荐，需自行加防护）
- 防火墙放行 `7860`（或自定义端口），并在云安全组开放。
- 建议配合反向代理（nginx/caddy）加 Basic Auth/Token；若仅测试可临时放行，结束后关闭端口。

## HTTP/API 调用示例（Gradio 内置 REST）
Gradio 提供 `/run/predict` 风格的接口。以 `gradio_vllm_demo.py` 的单图推理为例：
```bash
curl -X POST "http://localhost:7860/run/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "data": [
      "data:image/png;base64,<BASE64_IMAGE>",  // 上传图像
      "Free OCR",           // prompt_type（或 "Markdown Conversion" / "Custom"）
      "",                   // custom_prompt（仅 Custom 时填写）
      "Base",               // Size preset（建议 Base 或 Large）
      true,                 // crop_mode（随预设调整）
      4,                    // max_num_seqs（并发）
      0.85,                 // gpu_memory_utilization
      16384                 // max_tokens
    ]
  }'
```
- 不同 demo 版本字段顺序可能略有调整，可在浏览器 F12 Network 中观察 `/run/predict` 请求体获取精确参数。
- 返回 JSON 中通常包含 `data` 字段，含识别结果文本。

### Python 客户端示例
```python
import base64, json, requests

server = "http://localhost:7860"  # 若用 SSH 本地转发则保持 localhost:9000
with open("test.png", "rb") as f:
    img_b64 = base64.b64encode(f.read()).decode()

payload = {
    "data": [
        f"data:image/png;base64,{img_b64}",  # image
        "Free OCR",                         # prompt_type
        "",                                 # custom_prompt
        "Base",                             # model_size
        True,                               # crop_mode
        4,                                  # max_num_seqs
        0.85,                               # gpu_memory_utilization
        16384                               # max_tokens
    ]
}
resp = requests.post(f"{server}/run/predict", json=payload, timeout=300)
resp.raise_for_status()
print(json.dumps(resp.json(), ensure_ascii=False, indent=2))
```

## 常见问题
- 无法访问：检查 SSH 隧道是否存活，或端口是否被防火墙/安全组阻断。
- 显存不足：在 `DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py` 调低 `gpu_memory_utilization`、`max_num_seqs`，或选择较小的 Size 预设。
- 未启用离线：确保设置 `HF_HUB_OFFLINE=1` 与本地权重/分词器路径可读。
